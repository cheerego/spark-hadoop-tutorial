### 什么是Spark SQL？
Spark SQL 是Spark 1.0.0版本中新加入的组件，是Spark生态系统中最活跃的组件之一。它能够利用Spark进行结构化数据存储和操作。
提到Spark SQL，就不得不提到Hive和Shark，Hive是Shark的前身，Shark是Spark SQL的前身。根据伯克利实验室提供的数据，Shark基于内存计算的性能上比Hive高出100倍，即使是基于磁盘计算，他的性能也比Hive高出10倍，而Spark SQL的性能比Shark又高出一到两个数量级。

### Spark SQL的出现时解决什么问题？
首先就得说说Hive是解决什么问题的：`（个人观点）对于开发者来说编写大数据分析和存储的程序，面临着理解MapReduce计算模型和编写开发代码实现业务逻辑等困难。如果说实现一个功能数据查询的功能不使用Spark SQL需要30行代码，可能使用了Spark SQL不需要10行，极大的简化了编程中业务逻辑实现的困难`。相对于Hive本身而言，如果他要改进效率，最关键的就是替换一个比MapReduce更高效的计算引擎，而Spark刚好可以满足这一点。


### 使用Spark的原因
- 易用性与用户习惯。在过去很多年中，有大批程序员的工作室围绕DB+应用的架构来做的，因为SQL的易用性提升了应用的开发效率。程序员已经习惯了采用业务逻辑代码调用SQL的模式去写程序，惯性的力量是强大的，如果还能用原有的方式解决现有的大数据问题，何乐而不为呢？提供SQL和JDBC的支持会让传统用户像以前一样书写程序，大大减少迁移成本。
- 生态系统的力量。很多系统软件性能很好，但未取得成功便没落了，很大程度上是由于生态系统问题。传统的SQL在JDBC，ODBC，SQL的各种标准下形成了以整套成熟的生态系统，很多应用组件和工具可以迁移使用，像一些可视化工具、数据分析工具等，原有企业的IT工具可以无缝过度。
- 数据解耦。Spark SQL正在扩展支持多种持久化层，用户可以使用原有的持久化存储数据，但是也可以体验和迁移到Spark SQL提供的数据分析环境下分析Big Data。