### 启动spark shell
```scala
val listRDD = sc.parallelize(List(6,8,12,34,23))
val filterRDD = listRDD.filter(_>10)
filterRDD.collect
```
Resualt:
`res0: Array[Int]=Array(12,34,23)`

### 分析:
通过SparkContext提供的`parallelize`方法把scala的集合转化为RDD,然后对RDD里的数据进行filter操作,然后通过collect方法将filterRDD进行toArray的操作。
___
关于`parallelize`方法,通过这个方法将scala集合List装换成了ParallelCollectionRDD